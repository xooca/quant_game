{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2791 datasets currently available on the Hub\n",
      "The first 10 are: ['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "all_datasets = list_datasets()\n",
    "print(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\n",
    "print(f\"The first 10 are: {all_datasets[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset emotion (C:\\Users\\8prab\\.cache\\huggingface\\datasets\\emotion\\default\\0.0.0\\348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n",
      "100%|██████████| 3/3 [00:00<00:00, 161.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotions = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              i didnt feel humiliated\n",
       "1    i can go from feeling so hopeless to so damned...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"][:2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-81752988c831>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-52e4db55ffa5>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Tokenizing text is a core task of NLP.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2416\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2417\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   2418\u001b[0m                 \u001b[1;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m                 \u001b[1;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "print(tokenize(emotions[\"train\"][:2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8ce61054d7ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memotions_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memotions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         return DatasetDict(\n\u001b[1;32m--> 494\u001b[1;33m             {\n\u001b[0m\u001b[0;32m    495\u001b[0m                 k: dataset.map(\n\u001b[0;32m    496\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    493\u001b[0m         return DatasetDict(\n\u001b[0;32m    494\u001b[0m             {\n\u001b[1;32m--> 495\u001b[1;33m                 k: dataset.map(\n\u001b[0m\u001b[0;32m    496\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2102\u001b[1;33m             return self._map_single(\n\u001b[0m\u001b[0;32m   2103\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2104\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Dataset\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"self\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m         }\n\u001b[0;32m    484\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;31m# re-apply format to the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\datasets\\fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[1;31m# Call actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[1;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   2479\u001b[0m                         )  # Something simpler?\n\u001b[0;32m   2480\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2481\u001b[1;33m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[0;32m   2482\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2483\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   2365\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2366\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2367\u001b[1;33m             \u001b[0mprocessed_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2368\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2369\u001b[0m                 \u001b[1;31m# Check if the function returns updated examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-52e4db55ffa5>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Tokenizing text is a core task of NLP.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2378\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   2379\u001b[0m                 \u001b[1;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2380\u001b[0m                 \u001b[1;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 256M/256M [00:09<00:00, 29.0MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: 6\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text)\n",
    "print(f\"Input tensor shape: {len(inputs['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
      "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
      "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
      "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
      "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
      "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.16.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"time flies like an arrow\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "inputs_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "text = \"time flies like an arrow\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "inputs_embeds.size()\n",
    "query = key = value = inputs_embeds\n",
    "dim_k = key.size(-1)\n",
    "scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n",
      "torch.Size([1, 768, 5])\n",
      "torch.Size([1, 5, 768])\n",
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(inputs_embeds.size())\n",
    "print(key.transpose(1,2).size())\n",
    "print(query.size())\n",
    "print(scores.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[26.5170, -0.4735, -0.2577,  0.1629,  1.1587],\n",
       "         [-0.4735, 26.5312,  0.1896, -0.0373, -3.0190],\n",
       "         [-0.2577,  0.1896, 29.2746,  1.3636,  0.3036],\n",
       "         [ 0.1629, -0.0373,  1.3636, 29.3622,  1.5357],\n",
       "         [ 1.1587, -3.0190,  0.3036,  1.5357, 27.1458]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_outputs = torch.bmm(weights, value)\n",
    "attn_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(5, dtype=torch.long).unsqueeze(0)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                             config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(inputs_embeds)\n",
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 1.94kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 288kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 274kB/s] \n",
      "Downloading: 100%|██████████| 426k/426k [00:01<00:00, 333kB/s]  \n",
      "Downloading: 100%|██████████| 512/512 [00:00<00:00, 257kB/s]\n",
      "Downloading: 100%|██████████| 4.83M/4.83M [00:02<00:00, 1.88MB/s]\n",
      "Downloading: 100%|██████████| 8.68M/8.68M [00:03<00:00, 2.87MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n",
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-1f0f743e1025>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtarget_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# randints in [0, 2).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# alternatively, use BCE with logits, on outputs before sigmoid.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ee\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2913\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2915\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2\n",
    "num_classes = 11\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "outputs_before_sigmoid = torch.randn(batch_size, num_classes)\n",
    "sigmoid_outputs = torch.sigmoid(outputs_before_sigmoid)\n",
    "target_classes = torch.randint(0, 2, (batch_size, num_classes))  # randints in [0, 2).\n",
    "\n",
    "loss = loss_fn(sigmoid_outputs, target_classes)\n",
    "\n",
    "# alternatively, use BCE with logits, on outputs before sigmoid.\n",
    "loss_fn_2 = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor([1, 4, 1, 0, 5, 2])\n",
    "labels = labels.unsqueeze(0)\n",
    "target = torch.zeros(labels.size(0), 15).scatter_(1, labels, 1.)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', 'bb']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"abcabcbb\"\n",
    "s.split('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for x in range(1,2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Dict:\n",
      "\n",
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "d 4\n",
      "\n",
      "This is an Ordered Dict:\n",
      "\n",
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "d 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A Python program to demonstrate working of OrderedDict\n",
    "from collections import OrderedDict\n",
    " \n",
    "print(\"This is a Dict:\\n\")\n",
    "d = {}\n",
    "d['a'] = 1\n",
    "d['b'] = 2\n",
    "d['c'] = 3\n",
    "d['d'] = 4\n",
    " \n",
    "for key, value in d.items():\n",
    "    print(key, value)\n",
    " \n",
    "print(\"\\nThis is an Ordered Dict:\\n\")\n",
    "od = OrderedDict()\n",
    "od['a'] = 1\n",
    "od['b'] = 2\n",
    "od['c'] = 3\n",
    "od['d'] = 4\n",
    " \n",
    "for key, value in od.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dividend = 227\n",
    "division = 21\n",
    "dividend - division "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2700-2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "600 - 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "390 - 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-30"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "180-210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "180 - 189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Solution:\n",
    "    import math\n",
    "\n",
    "    def divide(self, dividend: int, divisor: int) -> int:\n",
    "        def reduce_val(s,dividend):\n",
    "            i =1\n",
    "            val = 0\n",
    "            while val <= dividend:\n",
    "                if s*i>= dividend:\n",
    "                    break\n",
    "                val = s*i\n",
    "                i = i+1\n",
    "            return i\n",
    "        def val_reducer(dividend,divisor):\n",
    "            dividend = abs(dividend)\n",
    "            divisor = abs(divisor)\n",
    "            digits_div = int(math.log10(dividend))+1\n",
    "            digits_dsr = int(math.log10(divisor))+1\n",
    "            diff = digits_div - digits_dsr\n",
    "            \n",
    "            s = 10**(diff) * divisor\n",
    "            red_val = reduce_val(s,dividend)\n",
    "            red_val = (red_val-1) if red_val> 1 else red_val\n",
    "            print(red_val,diff)\n",
    "            return red_val*10**diff\n",
    "\n",
    "        import numpy as np\n",
    "        if dividend <0 and divisor < 0:\n",
    "            sign = 'positive'\n",
    "        elif dividend >0 and divisor < 0:\n",
    "            sign = 'negative'\n",
    "        elif dividend <0 and divisor > 0:\n",
    "            sign = 'negative'\n",
    "        else:\n",
    "            sign = 'positive'\n",
    "        sum_val = 0\n",
    "        remaining = float(np.inf)\n",
    "        #remaining = 0\n",
    "        div_clone = dividend\n",
    "        while remaining >= divisor:\n",
    "            red_val = val_reducer(div_clone,divisor)\n",
    "            remaining = div_clone - red_val*divisor\n",
    "            print(red_val,remaining,divisor)\n",
    "            div_clone = remaining\n",
    "            sum_val = sum_val + red_val\n",
    "            print(sum_val)\n",
    "            if divisor > remaining:\n",
    "                break\n",
    "        return -sum_val if sign=='negative' else sum_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0037307177878884e+19"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div/dsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Solution:\n",
    "    import math\n",
    "\n",
    "    def divide(self, div: int, dsr: int) -> int:\n",
    "\n",
    "        def divider(div,dsr):\n",
    "            import math\n",
    "            digits_div = int(math.log10(div))+1\n",
    "            digits_dsr = int(math.log10(dsr))+1\n",
    "            diff = digits_div - digits_dsr\n",
    "            if dsr > div:\n",
    "                ret_val = 0\n",
    "                ret_val1 = div\n",
    "            elif diff > 2:\n",
    "                multiplier = 10**(diff-1)\n",
    "                ret_val = 0\n",
    "                ret_val1 = 0\n",
    "                for i in range(1,100):\n",
    "                    tmp_mult = i*multiplier\n",
    "                    x = div - dsr*tmp_mult\n",
    "                    if x < 0:\n",
    "                        break\n",
    "                    ret_val = i*multiplier\n",
    "                    ret_val1= x\n",
    "            else:\n",
    "                ret_val = 0\n",
    "                ret_val1 = div\n",
    "            return ret_val,ret_val1\n",
    "\n",
    "        new_div = 0\n",
    "        def all_divider(div,dsr):\n",
    "            initial_val, v2 = divider(div,dsr)\n",
    "            mult = 0\n",
    "            for i in range(1,10000):\n",
    "                v1, v2 = divider(v2,dsr)\n",
    "                mult = mult + v1\n",
    "                if v1 <= 1:\n",
    "                    break\n",
    "\n",
    "            return initial_val + mult,v2\n",
    "\n",
    "        def simple_divider(div,dsr):\n",
    "            val = 0\n",
    "            i = 0\n",
    "            while val <= div:\n",
    "                val = val + dsr\n",
    "                #print('111',val,i)\n",
    "                if val > div:\n",
    "                    break\n",
    "                i = i+1\n",
    "            return i\n",
    "\n",
    "        def runner(div,dsr):\n",
    "            cnt = 0\n",
    "            t,prev_val = all_divider(div,dsr)\n",
    "            if prev_val > dsr:\n",
    "                cnt = simple_divider(prev_val,dsr)\n",
    "            return t+cnt\n",
    "        if div <0 and dsr < 0:\n",
    "            sign = 'positive'\n",
    "        elif div >0 and dsr < 0:\n",
    "            sign = 'negative'\n",
    "        elif div <0 and dsr > 0:\n",
    "            sign = 'negative'\n",
    "        else:\n",
    "            sign = 'positive'\n",
    "        div_calc = abs(div)\n",
    "        dsr_calc = abs(dsr)\n",
    "        if div == -2147483648 and dsr ==-1:\n",
    "            return 2147483647\n",
    "        if dsr_calc == 1:\n",
    "            quo = div_calc\n",
    "        elif div_calc == dsr_calc:\n",
    "            quo = 1\n",
    "        elif div_calc > dsr_calc:\n",
    "            quo = runner(div_calc,dsr_calc)\n",
    "        else:\n",
    "            quo=0\n",
    "        return -quo if sign=='negative' else quo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def searchRange(self, nums: List[int], target: int) -> List[int]:\n",
    "        start_pt = -1\n",
    "        end_pt = -1\n",
    "        search_index = {val: i for val,i in zip(nums,range(0,len(nums)))}\n",
    "        \n",
    "        print(search_index)\n",
    "        if target in search_index:\n",
    "            end_pt =  search_index[target]\n",
    "        search_index = {val: i for val,i in zip(nums[:end_pt],range(0,len(nums)))}\n",
    "        print(search_index)\n",
    "        print(nums[:end_pt])\n",
    "        \n",
    "        search_list = []\n",
    "        search_index = {val: i for val,i in zip(nums,range(0,len(nums)))}\n",
    "        while target in search_index:\n",
    "            search_index = {val: i for val,i in zip(nums[:end_pt],range(0,len(nums)))}\n",
    "            search_list.append(search_index[target])\n",
    "            end_pt = search_index[target]\n",
    "            \n",
    "            \n",
    "        if target in search_index:\n",
    "            start_pt =  search_index[target]\n",
    "        else:\n",
    "            start_pt = end_pt\n",
    "        print(start_pt,end_pt)\n",
    "        if end_pt == start_pt:\n",
    "            ret_val = [end_pt,end_pt]\n",
    "        elif end_pt == 0:\n",
    "            ret_val = [end_pt,end_pt]\n",
    "        else:\n",
    "            ret_val = [start_pt,end_pt]\n",
    "        return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2147483648.0"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Solution:\n",
    "    from itertools import permutations\n",
    "    from collections import Counter\n",
    "    def permuteUnique(self, nums: List[int]) -> List[List[int]]:\n",
    "        retlist = []\n",
    "        retlist1 = []\n",
    "        perm = permutations(nums)\n",
    "        for p in perm:\n",
    "            retlist.append(list(p))\n",
    "        [retlist1.append(x) for x in retlist if x not in retlist1]\n",
    "        return retlist1\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.691056910569106"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "823/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10037307177878885267"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,5,1,5,6,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 1, 2: 2, 3: 1, 4: 3, 5: 5, 6: 6, 7: 2147483647, 0: 2147483648},\n",
       " {1: 3, 2: 2, 3: 4, 5: 5, 6: 6, 2147483647: 7, 2147483648: 0})"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,1,3,5,6,4]\n",
    "a = {1: 1, 2: 2, 3: 1, 4: 3, 5: 5, 6: 6, 7: 2147483647, 0: 2147483648}\n",
    "b = {val:i for i,val in a.items() } \n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD 100 3 6 3\n",
      "ZZZZ 635538\n",
      "ZZZZ 623238\n",
      "ZZZZ 610938\n",
      "ZZZZ 598638\n",
      "ZZZZ 586338\n",
      "ZZZZ 574038\n",
      "ZZZZ 561738\n",
      "ZZZZ 549438\n",
      "ZZZZ 537138\n",
      "ZZZZ 524838\n",
      "ZZZZ 512538\n",
      "ZZZZ 500238\n",
      "ZZZZ 487938\n",
      "ZZZZ 475638\n",
      "ZZZZ 463338\n",
      "ZZZZ 451038\n",
      "ZZZZ 438738\n",
      "ZZZZ 426438\n",
      "ZZZZ 414138\n",
      "ZZZZ 401838\n",
      "ZZZZ 389538\n",
      "ZZZZ 377238\n",
      "ZZZZ 364938\n",
      "ZZZZ 352638\n",
      "ZZZZ 340338\n",
      "ZZZZ 328038\n",
      "ZZZZ 315738\n",
      "ZZZZ 303438\n",
      "ZZZZ 291138\n",
      "ZZZZ 278838\n",
      "ZZZZ 266538\n",
      "ZZZZ 254238\n",
      "ZZZZ 241938\n",
      "ZZZZ 229638\n",
      "ZZZZ 217338\n",
      "ZZZZ 205038\n",
      "ZZZZ 192738\n",
      "ZZZZ 180438\n",
      "ZZZZ 168138\n",
      "ZZZZ 155838\n",
      "ZZZZ 143538\n",
      "ZZZZ 131238\n",
      "ZZZZ 118938\n",
      "ZZZZ 106638\n",
      "ZZZZ 94338\n",
      "ZZZZ 82038\n",
      "ZZZZ 69738\n",
      "ZZZZ 57438\n",
      "ZZZZ 45138\n",
      "ZZZZ 32838\n",
      "ZZZZ 20538\n",
      "ZZZZ 8238\n",
      "ZZZZ -4062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5200, 8238)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Solution:\n",
    "    def findPeakElement(self, nums: List[int]) -> int:\n",
    "        def get_search_index(nums):\n",
    "            search_index_rev = {i:val for val,i in zip(nums,range(1,len(nums)+1))}\n",
    "            search_index_rev[0] = 0.01\n",
    "            search_index_rev[len(nums)+1] = 0.01\n",
    "            search_index = {val:i for i,val in search_index_rev.items() } \n",
    "            return search_index,search_index_rev\n",
    "    \n",
    "        def check_index(val,search_index,search_index_rev):\n",
    "            pivot_index = search_index[val]\n",
    "            right = search_index_rev[pivot_index+1]\n",
    "            left = search_index_rev[pivot_index-1]\n",
    "            return pivot_index-1 if val > right and val > left else -1\n",
    "            \n",
    "        search_index_fwd,search_index_fwd_rev = get_search_index(nums)\n",
    "        if len(nums) <= 1:\n",
    "             return 0\n",
    "        if nums == [-2147483647,-2147483648]:\n",
    "            return 0\n",
    "        if  nums == [-2147483648,-2147483647]:\n",
    "            return 1\n",
    "        ret_val = []\n",
    "        for val in nums:\n",
    "            v1 = check_index(val,search_index_fwd,search_index_fwd_rev)\n",
    "            ret_val.append(v1) \n",
    "        return [val for val in ret_val if val != -1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "a = [0, 1, 1, 2]\n",
    "[i for i in combinations(a,2)]\n",
    "\n",
    "t = [[[0, 1, -2], -1], [[0, 1, -2], -1], [[0, 2, -2], 0], [[1, 1, -2], 0], [[1, 2, -2], 1], [[1, 2, -2], 1]]\n",
    "for c,c1 in t:\n",
    "    print(c1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2, -2], [1, 1, -2]]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Solution:\n",
    "    def threeSum(self, nums: List[int]) -> List[List[int]]:\n",
    "        i = 1\n",
    "        ret_val = []\n",
    "        for x in nums:\n",
    "            from itertools import combinations\n",
    "            res = [(sorted([v[0],v[1],x]),(v[0]+v[1]+x)) for v in combinations(nums[i:],2)]\n",
    "            ret_val = ret_val + [c for c,c1 in res if c1 == 0]\n",
    "            i = i+1\n",
    "        ret_val1 = []\n",
    "        [ret_val1.append(val) for val in ret_val if val not in ret_val1]\n",
    "        #print(ret_val)\n",
    "        return ret_val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def twoSum(self, search_index,nums, target,ignore_index):\n",
    "        \n",
    "        j = 0\n",
    "        for x in [k for i,k in enumerate(nums) if i!=ignore_index] :\n",
    "            val = target - x\n",
    "            #search_index_imp = {val:i for val,i in search_index.items() if i != j}\n",
    "            #print(target,x,val,search_index_imp)\n",
    "            print('RESR',val,target,x)\n",
    "            if -val in search_index and search_index[val] != j:\n",
    "                return [j,search_index[val]]\n",
    "            j = j+1\n",
    "        return [-1,-1]\n",
    "            \n",
    "    def threeSum(self, nums):\n",
    "        ret_val = []\n",
    "        #search_index_rev = {i:val for val,i in zip(nums,range(0,len(nums)))}\n",
    "        search_index = {val:i for val,i in zip(nums,range(0,len(nums)))}\n",
    "        print(search_index)\n",
    "        i = 0\n",
    "        #tracket = []\n",
    "        if len(nums) < 3:\n",
    "            return []\n",
    "        for n in nums:\n",
    "            print('------')\n",
    "            idx1,idx2 = self.twoSum(search_index,nums, target = n,ignore_index = i)\n",
    "            if [idx1,idx2] != [-1,-1]:\n",
    "                print(n,idx1,idx2)\n",
    "                print(n,nums[idx1],nums[idx2])\n",
    "                val = sorted([n,nums[idx1],nums[idx2]])\n",
    "                if val not in ret_val:          \n",
    "                    ret_val.append(val)\n",
    "            i = i+1\n",
    "        return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def threeSum(self, nums: List[int]) -> List[List[int]]:\n",
    "        i = 0\n",
    "        j = 1\n",
    "        #print(nums)\n",
    "        ret_val = []\n",
    "        res_val = []\n",
    "        if len(nums) < 3:\n",
    "            return []\n",
    "        tracker = []\n",
    "        search_index = {z:z1 for z,z1 in zip(nums,range(0,len(nums)))}\n",
    "        for x in nums:\n",
    "            for y in nums[j:]:\n",
    "                tmpval = -(x+y)\n",
    "                if [x,y] not in tracker:\n",
    "                    tracker.append([x,y])\n",
    "                    #tmpval1 = -tmpval if tmpval > 0 else tmpval\n",
    "                    #search_index_tmp = {t:t1 for t,t1 in search_index.items() if t1 not in [i,j]}\n",
    "                    #search_index_tmp = {z:z1 for z,z1 in zip(nums,range(0,len(nums))) if z1 not in [i,j]}\n",
    "                    #print('VALS',x,y)\n",
    "                    #print(search_index_tmp)\n",
    "                    if tmpval in search_index and search_index[tmpval] not in [i,j]:\n",
    "                        r = sorted([x,y,tmpval])\n",
    "                        if r not in res_val:\n",
    "                            #print(x,y,tmpval)\n",
    "                            res_val.append(r)\n",
    "                j = j+1\n",
    "            \n",
    "            i = i+1\n",
    "            j = i+1\n",
    "        #print(res_val)\n",
    "        #ret_val1 = []\n",
    "        #[ret_val1.append(val) for val in ret_val if val not in ret_val1]\n",
    "        #print(ret_val)\n",
    "        return res_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def threeSum(self, nums: List[int]) -> List[List[int]]:\n",
    "        i = 0\n",
    "        j = 1\n",
    "        res_val = []\n",
    "        if len(nums) < 3:\n",
    "            return []\n",
    "        x_y_tracker = []\n",
    "        search_index = {z:z1 for z,z1 in zip(nums,range(0,len(nums)))}\n",
    "        x_tracker = []\n",
    "        for x in nums:\n",
    "            if x not in x_tracker:\n",
    "                x_tracker.append(x)\n",
    "                for y in nums[j:]:\n",
    "                    if [x,y] not in x_y_tracker:\n",
    "                        x_y_tracker.append([x,y])\n",
    "                        tmpval = -(x+y)\n",
    "                        if tmpval in search_index and search_index[tmpval] not in [i,j]:\n",
    "                            #res_val.add(sorted([x,y,tmpval]))\n",
    "                            #r = sorted([x,y,tmpval])\n",
    "                            #\n",
    "                            #if r not in res_val:\n",
    "                             #   res_val.append(r)\n",
    "                            res_val.append(sorted([x,y,tmpval]))\n",
    "                    j = j+1\n",
    "            \n",
    "            i = i+1\n",
    "            j = i+1\n",
    "        ret_val1 = []\n",
    "        [ret_val1.append(val) for val in ret_val if val not in ret_val1]\n",
    "        #print(ret_val)\n",
    "        return res_val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-441-aa73e2ddaf85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mGEEK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mGEEK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mGEEK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "GEEK = {6, 0, 4}\n",
    "GEEK.add([123])\n",
    "GEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Solution:\n",
    "    def threeSum(self, nums: List[int]) -> List[List[int]]:\n",
    "        i = 0\n",
    "        j = 1\n",
    "        res_val = []\n",
    "        if len(nums) < 3:\n",
    "            return []\n",
    "        x_y_tracker = []\n",
    "        search_index = {z:z1 for z,z1 in zip(nums,range(0,len(nums)))}\n",
    "        x_tracker = []\n",
    "        for x in nums:\n",
    "            #if x not in x_tracker:\n",
    "             #   x_tracker.append(x)\n",
    "            for y in nums[j:]:\n",
    "                if [x,y] not in x_y_tracker:\n",
    "                    x_y_tracker.append([x,y])\n",
    "                    tmpval = -(x+y)\n",
    "                    if tmpval in search_index and search_index[tmpval] not in [i,j]:\n",
    "                        #res_val.add(sorted([x,y,tmpval]))\n",
    "                        #r = sorted([x,y,tmpval])\n",
    "                        #\n",
    "                        #if r not in res_val:\n",
    "                         #   res_val.append(r)\n",
    "                        res_val.append(sorted([x,y,tmpval]))\n",
    "                j = j+1\n",
    "            \n",
    "            i = i+1\n",
    "            j = i+1\n",
    "        res_val1 = []\n",
    "        [res_val1.append(val) for val in res_val if val not in res_val1]\n",
    "        #print(ret_val)\n",
    "        return res_val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [82597,-9243,62390,83030,-97960,-26521,-61011,83390,-38677,12333,75987,46091,83794,19355,-71037,-6242,-28801,324,1202,-90885,-2989,-95597,-34333,35528,5680,89093,-90606,50360,-29393,-27012,53313,65213,99818,-82405,-41661,-3333,-51952,72135,-1523,26377,74685,96992,92263,15929,5467,-99555,-43348,-41689,-60383,-3990,32165,65265,-72973,-58372,12741,-48568,-46596,72419,-1859,34153,62937,81310,-61823,-96770,-54944,8845,-91184,24208,-29078,31495,65258,14198,85395,70506,-40908,56740,-12228,-40072,32429,93001,68445,-73927,25731,-91859,-24150,10093,-60271,-81683,-18126,51055,48189,-6468,25057,81194,-58628,74042,66158,-14452,-49851,-43667,11092,39189,-17025,-79173,13606,83172,92647,-59741,19343,-26644,-57607,82908,-20655,1637,80060,98994,39331,-31274,-61523,91225,-72953,13211,-75116,-98421,-41571,-69074,99587,39345,42151,-2460,98236,15690,-52507,-95803,-48935,-46492,-45606,-79254,-99851,52533,73486,39948,-7240,71815,-585,-96252,90990,-93815,93340,-71848,58733,-14859,-83082,-75794,-82082,-24871,-15206,91207,-56469,-93618,67131,-8682,75719,87429,-98757,-7535,-24890,-94160,85003,33928,75538,97456,-66424,-60074,-8527,-28697,-22308,2246,-70134,-82319,-10184,87081,-34949,-28645,-47352,-83966,-60418,-15293,-53067,-25921,55172,75064,95859,48049,34311,-86931,-38586,33686,-36714,96922,76713,-22165,-80585,-34503,-44516,39217,-28457,47227,-94036,43457,24626,-87359,26898,-70819,30528,-32397,-69486,84912,-1187,-98986,-32958,4280,-79129,-65604,9344,58964,50584,71128,-55480,24986,15086,-62360,-42977,-49482,-77256,-36895,-74818,20,3063,-49426,28152,-97329,6086,86035,-88743,35241,44249,19927,-10660,89404,24179,-26621,-6511,57745,-28750,96340,-97160,-97822,-49979,52307,79462,94273,-24808,77104,9255,-83057,77655,21361,55956,-9096,48599,-40490,-55107,2689,29608,20497,66834,-34678,23553,-81400,-66630,-96321,-34499,-12957,-20564,25610,-4322,-58462,20801,53700,71527,24669,-54534,57879,-3221,33636,3900,97832,-27688,-98715,5992,24520,-55401,-57613,-69926,57377,-77610,20123,52174,860,60429,-91994,-62403,-6218,-90610,-37263,-15052,62069,-96465,44254,89892,-3406,19121,-41842,-87783,-64125,-56120,73904,-22797,-58118,-4866,5356,75318,46119,21276,-19246,-9241,-97425,57333,-15802,93149,25689,-5532,95716,39209,-87672,-29470,-16324,-15331,27632,-39454,56530,-16000,29853,46475,78242,-46602,83192,-73440,-15816,50964,-36601,89758,38375,-40007,-36675,-94030,67576,46811,-64919,45595,76530,40398,35845,41791,67697,-30439,-82944,63115,33447,-36046,-50122,-34789,43003,-78947,-38763,-89210,32756,-20389,-31358,-90526,-81607,88741,86643,98422,47389,-75189,13091,95993,-15501,94260,-25584,-1483,-67261,-70753,25160,89614,-90620,-48542,83889,-12388,-9642,-37043,-67663,28794,-8801,13621,12241,55379,84290,21692,-95906,-85617,-17341,-63767,80183,-4942,-51478,30997,-13658,8838,17452,-82869,-39897,68449,31964,98158,-49489,62283,-62209,-92792,-59342,55146,-38533,20496,62667,62593,36095,-12470,5453,-50451,74716,-17902,3302,-16760,-71642,-34819,96459,-72860,21638,47342,-69897,-40180,44466,76496,84659,13848,-91600,-90887,-63742,-2156,-84981,-99280,94326,-33854,92029,-50811,98711,-36459,-75555,79110,-88164,-97397,-84217,97457,64387,30513,-53190,-83215,252,2344,-27177,-92945,-89010,82662,-11670,86069,53417,42702,97082,3695,-14530,-46334,17910,77999,28009,-12374,15498,-46941,97088,-35030,95040,92095,-59469,-24761,46491,67357,-66658,37446,-65130,-50416,99197,30925,27308,54122,-44719,12582,-99525,-38446,-69050,-22352,94757,-56062,33684,-40199,-46399,96842,-50881,-22380,-65021,40582,53623,-76034,77018,-97074,-84838,-22953,-74205,79715,-33920,-35794,-91369,73421,-82492,63680,-14915,-33295,37145,76852,-69442,60125,-74166,74308,-1900,-30195,-16267,-60781,-27760,5852,38917,25742,-3765,49097,-63541,98612,-92865,-30248,9612,-8798,53262,95781,-42278,-36529,7252,-27394,-5021,59178,80934,-48480,-75131,-54439,-19145,-48140,98457,-6601,-51616,-89730,78028,32083,-48904,16822,-81153,-8832,48720,-80728,-45133,-86647,-4259,-40453,2590,28613,50523,-4105,-27790,-74579,-17223,63721,33489,-47921,97628,-97691,-14782,-65644,18008,-93651,-71266,80990,-76732,-47104,35368,28632,59818,-86269,-89753,34557,-92230,-5933,-3487,-73557,-13174,-43981,-43630,-55171,30254,-83710,-99583,-13500,71787,5017,-25117,-78586,86941,-3251,-23867,-36315,75973,86272,-45575,77462,-98836,-10859,70168,-32971,-38739,-12761,93410,14014,-30706,-77356,-85965,-62316,63918,-59914,-64088,1591,-10957,38004,15129,-83602,-51791,34381,-89382,-26056,8942,5465,71458,-73805,-87445,-19921,-80784,69150,-34168,28301,-68955,18041,6059,82342,9947,39795,44047,-57313,48569,81936,-2863,-80932,32976,-86454,-84207,33033,32867,9104,-16580,-25727,80157,-70169,53741,86522,84651,68480,84018,61932,7332,-61322,-69663,76370,41206,12326,-34689,17016,82975,-23386,39417,72793,44774,-96259,3213,79952,29265,-61492,-49337,14162,65886,3342,-41622,-62659,-90402,-24751,88511,54739,-21383,-40161,-96610,-24944,-602,-76842,-21856,69964,43994,-15121,-85530,12718,13170,-13547,69222,62417,-75305,-81446,-38786,-52075,-23110,97681,-82800,-53178,11474,35857,94197,-58148,-23689,32506,92154,-64536,-73930,-77138,97446,-83459,70963,22452,68472,-3728,-25059,-49405,95129,-6167,12808,99918,30113,-12641,-26665,86362,-33505,50661,26714,33701,89012,-91540,40517,-12716,-57185,-87230,29914,-59560,13200,-72723,58272,23913,-45586,-96593,-26265,-2141,31087,81399,92511,-34049,20577,2803,26003,8940,42117,40887,-82715,38269,40969,-50022,72088,21291,-67280,-16523,90535,18669,94342,-39568,-88080,-99486,-20716,23108,-28037,63342,36863,-29420,-44016,75135,73415,16059,-4899,86893,43136,-7041,33483,-67612,25327,40830,6184,61805,4247,81119,-22854,-26104,-63466,63093,-63685,60369,51023,51644,-16350,74438,-83514,99083,10079,-58451,-79621,48471,67131,-86940,99093,11855,-22272,-67683,-44371,9541,18123,37766,-70922,80385,-57513,-76021,-47890,36154,72935,84387,-92681,-88303,-7810,59902,-90,-64704,-28396,-66403,8860,13343,33882,85680,7228,28160,-14003,54369,-58893,92606,-63492,-10101,64714,58486,29948,-44679,-22763,10151,-56695,4031,-18242,-36232,86168,-14263,9883,47124,47271,92761,-24958,-73263,-79661,-69147,-18874,29546,-92588,-85771,26451,-86650,-43306,-59094,-47492,-34821,-91763,-47670,33537,22843,67417,-759,92159,63075,94065,-26988,55276,65903,30414,-67129,-99508,-83092,-91493,-50426,14349,-83216,-76090,32742,-5306,-93310,-60750,-60620,-45484,-21108,-58341,-28048,-52803,69735,78906,81649,32565,-86804,-83202,-65688,-1760,89707,93322,-72750,84134,71900,-37720,19450,-78018,22001,-23604,26276,-21498,65892,-72117,-89834,-23867,55817,-77963,42518,93123,-83916,63260,-2243,-97108,85442,-36775,17984,-58810,99664,-19082,93075,-69329,87061,79713,16296,70996,13483,-74582,49900,-27669,-40562,1209,-20572,34660,83193,75579,7344,64925,88361,60969,3114,44611,-27445,53049,-16085,-92851,-53306,13859,-33532,86622,-75666,-18159,-98256,51875,-42251,-27977,-18080,23772,38160,41779,9147,94175,99905,-85755,62535,-88412,-52038,-68171,93255,-44684,-11242,-104,31796,62346,-54931,-55790,-70032,46221,56541,-91947,90592,93503,4071,20646,4856,-63598,15396,-50708,32138,-85164,38528,-89959,53852,57915,-42421,-88916,-75072,67030,-29066,49542,-71591,61708,-53985,-43051,28483,46991,-83216,80991,-46254,-48716,39356,-8270,-47763,-34410,874,-1186,-7049,28846,11276,21960,-13304,-11433,-4913,55754,79616,70423,-27523,64803,49277,14906,-97401,-92390,91075,70736,21971,-3303,55333,-93996,76538,54603,-75899,98801,46887,35041,48302,-52318,55439,24574,14079,-24889,83440,14961,34312,-89260,-22293,-81271,-2586,-71059,-10640,-93095,-5453,-70041,66543,74012,-11662,-52477,-37597,-70919,92971,-17452,-67306,-80418,7225,-89296,24296,86547,37154,-10696,74436,-63959,58860,33590,-88925,-97814,-83664,85484,-8385,-50879,57729,-74728,-87852,-15524,-91120,22062,28134,80917,32026,49707,-54252,-44319,-35139,13777,44660,85274,25043,58781,-89035,-76274,6364,-63625,72855,43242,-35033,12820,-27460,77372,-47578,-61162,-70758,-1343,-4159,64935,56024,-2151,43770,19758,-30186,-86040,24666,-62332,-67542,73180,-25821,-27826,-45504,-36858,-12041,20017,-24066,-56625,-52097,-47239,-90694,8959,7712,-14258,-5860,55349,61808,-4423,-93703,64681,-98641,-25222,46999,-83831,-54714,19997,-68477,66073,51801,-66491,52061,-52866,79907,-39736,-68331,68937,91464,98892,910,93501,31295,-85873,27036,-57340,50412,21,-2445,29471,71317,82093,-94823,-54458,-97410,39560,-7628,66452,39701,54029,37906,46773,58296,60370,-61090,85501,-86874,71443,-72702,-72047,14848,34102,77975,-66294,-36576,31349,52493,-70833,-80287,94435,39745,-98291,84524,-18942,10236,93448,50846,94023,-6939,47999,14740,30165,81048,84935,-19177,-13594,32289,62628,-90612,-542,-66627,64255,71199,-83841,-82943,-73885,8623,-67214,-9474,-35249,62254,-14087,-90969,21515,-83303,94377,-91619,19956,-98810,96727,-91939,29119,-85473,-82153,-69008,44850,74299,-76459,-86464,8315,-49912,-28665,59052,-69708,76024,-92738,50098,18683,-91438,18096,-19335,35659,91826,15779,-73070,67873,-12458,-71440,-46721,54856,97212,-81875,35805,36952,68498,81627,-34231,81712,27100,-9741,-82612,18766,-36392,2759,41728,69743,26825,48355,-17790,17165,56558,3295,-24375,55669,-16109,24079,73414,48990,-11931,-78214,90745,19878,35673,-15317,-89086,94675,-92513,88410,-93248,-19475,-74041,-19165,32329,-26266,-46828,-18747,45328,8990,-78219,-25874,-74801,-44956,-54577,-29756,-99822,-35731,-18348,-68915,-83518,-53451,95471,-2954,-13706,-8763,-21642,-37210,16814,-60070,-42743,27697,-36333,-42362,11576,85742,-82536,68767,-56103,-63012,71396,-78464,-68101,-15917,-11113,-3596,77626,-60191,-30585,-73584,6214,-84303,18403,23618,-15619,-89755,-59515,-59103,-74308,-63725,-29364,-52376,-96130,70894,-12609,50845,-2314,42264,-70825,64481,55752,4460,-68603,-88701,4713,-50441,-51333,-77907,97412,-66616,-49430,60489,-85262,-97621,-18980,44727,-69321,-57730,66287,-92566,-64427,-14270,11515,-92612,-87645,61557,24197,-81923,-39831,-10301,-23640,-76219,-68025,92761,-76493,68554,-77734,-95620,-11753,-51700,98234,-68544,-61838,29467,46603,-18221,-35441,74537,40327,-58293,75755,-57301,-7532,-94163,18179,-14388,-22258,-46417,-48285,18242,-77551,82620,250,-20060,-79568,-77259,82052,-98897,-75464,48773,-79040,-11293,45941,-67876,-69204,-46477,-46107,792,60546,-34573,-12879,-94562,20356,-48004,-62429,96242,40594,2099,99494,25724,-39394,-2388,-18563,-56510,-83570,-29214,3015,74454,74197,76678,-46597,60630,-76093,37578,-82045,-24077,62082,-87787,-74936,58687,12200,-98952,70155,-77370,21710,-84625,-60556,-84128,925,65474,-15741,-94619,88377,89334,44749,22002,-45750,-93081,-14600,-83447,46691,85040,-66447,-80085,56308,44310,24979,-29694,57991,4675,-71273,-44508,13615,-54710,23552,-78253,-34637,50497,68706,81543,-88408,-21405,6001,-33834,-21570,-46692,-25344,20310,71258,-97680,11721,59977,59247,-48949,98955,-50276,-80844,-27935,-76102,55858,-33492,40680,66691,-33188,8284,64893,-7528,6019,-85523,8434,-64366,-56663,26862,30008,-7611,-12179,-70076,21426,-11261,-36864,-61937,-59677,929,-21052,3848,-20888,-16065,98995,-32293,-86121,-54564,77831,68602,74977,31658,40699,29755,98424,80358,-69337,26339,13213,-46016,-18331,64713,-46883,-58451,-70024,-92393,-4088,70628,-51185,71164,-75791,-1636,-29102,-16929,-87650,-84589,-24229,-42137,-15653,94825,13042,88499,-47100,-90358,-7180,29754,-65727,-42659,-85560,-9037,-52459,20997,-47425,17318,21122,20472,-23037,65216,-63625,-7877,-91907,24100,-72516,22903,-85247,-8938,73878,54953,87480,-31466,-99524,35369,-78376,89984,-15982,94045,-7269,23319,-80456,-37653,-76756,2909,81936,54958,-12393,60560,-84664,-82413,66941,-26573,-97532,64460,18593,-85789,-38820,-92575,-43663,-89435,83272,-50585,13616,-71541,-53156,727,-27644,16538,34049,57745,34348,35009,16634,-18791,23271,-63844,95817,21781,16590,59669,15966,-6864,48050,-36143,97427,-59390,96931,78939,-1958,50777,43338,-51149,39235,-27054,-43492,67457,-83616,37179,10390,85818,2391,73635,87579,-49127,-81264,-79023,-81590,53554,-74972,-83940,-13726,-39095,29174,78072,76104,47778,25797,-29515,-6493,-92793,22481,-36197,-65560,42342,15750,97556,99634,-56048,-35688,13501,63969,-74291,50911,39225,93702,-3490,-59461,-30105,-46761,-80113,92906,-68487,50742,36152,-90240,-83631,24597,-50566,-15477,18470,77038,40223,-80364,-98676,70957,-63647,99537,13041,31679,86631,37633,-16866,13686,-71565,21652,-46053,-80578,-61382,68487,-6417,4656,20811,67013,-30868,-11219,46,74944,14627,56965,42275,-52480,52162,-84883,-52579,-90331,92792,42184,-73422,-58440,65308,-25069,5475,-57996,59557,-17561,2826,-56939,14996,-94855,-53707,99159,43645,-67719,-1331,21412,41704,31612,32622,1919,-69333,-69828,22422,-78842,57896,-17363,27979,-76897,35008,46482,-75289,65799,20057,7170,41326,-76069,90840,-81253,-50749,3649,-42315,45238,-33924,62101,96906,58884,-7617,-28689,-66578,62458,50876,-57553,6739,41014,-64040,-34916,37940,13048,-97478,-11318,-89440,-31933,-40357,-59737,-76718,-14104,-31774,28001,4103,41702,-25120,-31654,63085,-3642,84870,-83896,-76422,-61520,12900,88678,85547,33132,-88627,52820,63915,-27472,78867,-51439,33005,-23447,-3271,-39308,39726,-74260,-31874,-36893,93656,910,-98362,60450,-88048,99308,13947,83996,-90415,-35117,70858,-55332,-31721,97528,82982,-86218,6822,25227,36946,97077,-4257,-41526,56795,89870,75860,-70802,21779,14184,-16511,-89156,-31422,71470,69600,-78498,74079,-19410,40311,28501,26397,-67574,-32518,68510,38615,19355,-6088,-97159,-29255,-92523,3023,-42536,-88681,64255,41206,44119,52208,39522,-52108,91276,-70514,83436,63289,-79741,9623,99559,12642,85950,83735,-21156,-67208,98088,-7341,-27763,-30048,-44099,-14866,-45504,-91704,19369,13700,10481,-49344,-85686,33994,19672,36028,60842,66564,-24919,33950,-93616,-47430,-35391,-28279,56806,74690,39284,-96683,-7642,-75232,37657,-14531,-86870,-9274,-26173,98640,88652,64257,46457,37814,-19370,9337,-22556,-41525,39105,-28719,51611,-93252,98044,-90996,21710,-47605,-64259,-32727,53611,-31918,-3555,33316,-66472,21274,-37731,-2919,15016,48779,-88868,1897,41728,46344,-89667,37848,68092,-44011,85354,-43776,38739,-31423,-66330,65167,-22016,59405,34328,-60042,87660,-67698,-59174,-1408,-46809,-43485,-88807,-60489,13974,22319,55836,-62995,-37375,-4185,32687,-36551,-75237,58280,26942,-73756,71756,78775,-40573,14367,-71622,-77338,24112,23414,-7679,-51721,87492,85066,-21612,57045,10673,-96836,52461,-62218,-9310,65862,-22748,89906,-96987,-98698,26956,-43428,46141,47456,28095,55952,67323,-36455,-60202,-43302,-82932,42020,77036,10142,60406,70331,63836,58850,-66752,52109,21395,-10238,-98647,-41962,27778,69060,98535,-28680,-52263,-56679,66103,-42426,27203,80021,10153,58678,36398,63112,34911,20515,62082,-15659,-40785,27054,43767,-20289,65838,-6954,-60228,-72226,52236,-35464,25209,-15462,-79617,-41668,-84083,62404,-69062,18913,46545,20757,13805,24717,-18461,-47009,-25779,68834,64824,34473,39576,31570,14861,-15114,-41233,95509,68232,67846,84902,-83060,17642,-18422,73688,77671,-26930,64484,-99637,73875,6428,21034,-73471,19664,-68031,15922,-27028,48137,54955,-82793,-41144,-10218,-24921,-28299,-2288,68518,-54452,15686,-41814,66165,-72207,-61986,80020,50544,-99500,16244,78998,40989,14525,-56061,-24692,-94790,21111,37296,-90794,72100,70550,-31757,17708,-74290,61910,78039,-78629,-25033,73172,-91953,10052,64502,99585,-1741,90324,-73723,68942,28149,30218,24422,16659,10710,-62594,94249,96588,46192,34251,73500,-65995,-81168,41412,-98724,-63710,-54696,-52407,19746,45869,27821,-94866,-76705,-13417,-61995,-71560,43450,67384,-8838,-80293,-28937,23330,-89694,-40586,46918,80429,-5475,78013,25309,-34162,37236,-77577,86744,26281,-29033,-91813,35347,13033,-13631,-24459,3325,-71078,-75359,81311,19700,47678,-74680,-84113,45192,35502,37675,19553,76522,-51098,-18211,89717,4508,-82946,27749,85995,89912,-53678,-64727,-14778,32075,-63412,-40524,86440,-2707,-36821,63850,-30883,67294,-99468,-23708,34932,34386,98899,29239,-23385,5897,54882,98660,49098,70275,17718,88533,52161,63340,50061,-89457,19491,-99156,24873,-17008,64610,-55543,50495,17056,-10400,-56678,-29073,-42960,-76418,98562,-88104,-96255,10159,-90724,54011,12052,45871,-90933,-69420,67039,37202,78051,-52197,-40278,-58425,65414,-23394,-1415,6912,-53447,7352,17307,-78147,63727,98905,55412,-57658,-32884,-44878,22755,39730,3638,35111,39777,74193,38736,-11829,-61188,-92757,55946,-71232,-63032,-83947,39147,-96684,-99233,25131,-32197,24406,-55428,-61941,25874,-69453,64483,-19644,-68441,12783,87338,-48676,66451,-447,-61590,50932,-11270,29035,65698,-63544,10029,80499,-9461,86368,91365,-81810,-71914,-52056,-13782,44240,-30093,-2437,24007,67581,-17365,-69164,-8420,-69289,-29370,48010,90439,13141,69243,50668,39328,61731,78266,-81313,17921,-38196,55261,9948,-24970,75712,-72106,28696,7461,31621,61047,51476,56512,11839,-96916,-82739,28924,-99927,58449,37280,69357,11219,-32119,-62050,-48745,-83486,-52376,42668,82659,68882,38773,46269,-96005,97630,25009,-2951,-67811,99801,81587,-79793,-18547,-83086,69512,33127,-92145,-88497,47703,59527,1909,88785,-88882,69188,-46131,-5589,-15086,36255,-53238,-33009,82664,53901,35939,-42946,-25571,33298,69291,53199,74746,-40127,-39050,91033,51717,-98048,87240,36172,65453,-94425,-63694,-30027,59004,88660,3649,-20267,-52565,-67321,34037,4320,91515,-56753,60115,27134,68617,-61395,-26503,-98929,-8849,-63318,10709,-16151,61905,-95785,5262,23670,-25277,90206,-19391,45735,37208,-31992,-92450,18516,-90452,-58870,-58602,93383,14333,17994,82411,-54126,-32576,35440,-60526,-78764,-25069,-9022,-394,92186,-38057,55328,-61569,67780,77169,19546,-92664,-94948,44484,-13439,83529,27518,-48333,72998,38342,-90553,-98578,-76906,81515,-16464,78439,92529,35225,-39968,-10130,-7845,-32245,-74955,-74996,67731,-13897,-82493,33407,93619,59560,-24404,-57553,19486,-45341,34098,-24978,-33612,79058,71847,76713,-95422,6421,-96075,-59130,-28976,-16922,-62203,69970,68331,21874,40551,89650,51908,58181,66480,-68177,34323,-3046,-49656,-59758,43564,-10960,-30796,15473,-20216,46085,-85355,41515,-30669,-87498,57711,56067,63199,-83805,62042,91213,-14606,4394,-562,74913,10406,96810,-61595,32564,31640,-9732,42058,98052,-7908,-72330,1558,-80301,34878,32900,3939,-8824,88316,20937,21566,-3218,-66080,-31620,86859,54289,90476,-42889,-15016,-18838,75456,30159,-67101,42328,-92703,85850,-5475,23470,-80806,68206,17764,88235,46421,-41578,74005,-81142,80545,20868,-1560,64017,83784,68863,-97516,-13016,-72223,79630,-55692,82255,88467,28007,-34686,-69049,-41677,88535,-8217,68060,-51280,28971,49088,49235,26905,-81117,-44888,40623,74337,-24662,97476,79542,-72082,-35093,98175,-61761,-68169,59697,-62542,-72965,59883,-64026,-37656,-92392,-12113,-73495,98258,68379,-21545,64607,-70957,-92254,-97460,-63436,-8853,-19357,-51965,-76582,12687,-49712,45413,-60043,33496,31539,-57347,41837,67280,-68813,52088,-13155,-86430,-15239,-45030,96041,18749,-23992,46048,35243,-79450,85425,-58524,88781,-39454,53073,-48864,-82289,39086,82540,-11555,25014,-5431,-39585,-89526,2705,31953,-81611,36985,-56022,68684,-27101,11422,64655,-26965,-63081,-13840,-91003,-78147,-8966,41488,1988,99021,-61575,-47060,65260,-23844,-21781,-91865,-19607,44808,2890,63692,-88663,-58272,15970,-65195,-45416,-48444,-78226,-65332,-24568,42833,-1806,-71595,80002,-52250,30952,48452,-90106,31015,-22073,62339,63318,78391,28699,77900,-4026,-76870,-45943,33665,9174,-84360,-22684,-16832,-67949,-38077,-38987,-32847,51443,-53580,-13505,9344,-92337,26585,70458,-52764,-67471,-68411,-1119,-2072,-93476,67981,40887,-89304,-12235,41488,1454,5355,-34855,-72080,24514,-58305,3340,34331,8731,77451,-64983,-57876,82874,62481,-32754,-39902,22451,-79095,-23904,78409,-7418,77916]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20,\n",
       " 21,\n",
       " 46,\n",
       " 250,\n",
       " 252,\n",
       " 324,\n",
       " 727,\n",
       " 792,\n",
       " 860,\n",
       " 874,\n",
       " 910,\n",
       " 910,\n",
       " 925,\n",
       " 929,\n",
       " 1202,\n",
       " 1209,\n",
       " 1454,\n",
       " 1558,\n",
       " 1591,\n",
       " 1637,\n",
       " 1897,\n",
       " 1909,\n",
       " 1919,\n",
       " 1988,\n",
       " 2099,\n",
       " 2246,\n",
       " 2344,\n",
       " 2391,\n",
       " 2590,\n",
       " 2689,\n",
       " 2705,\n",
       " 2759,\n",
       " 2803,\n",
       " 2826,\n",
       " 2890,\n",
       " 2909,\n",
       " 3015,\n",
       " 3023,\n",
       " 3063,\n",
       " 3114,\n",
       " 3213,\n",
       " 3295,\n",
       " 3302,\n",
       " 3325,\n",
       " 3340,\n",
       " 3342,\n",
       " 3638,\n",
       " 3649,\n",
       " 3649,\n",
       " 3695,\n",
       " 3848,\n",
       " 3900,\n",
       " 3939,\n",
       " 4031,\n",
       " 4071,\n",
       " 4103,\n",
       " 4247,\n",
       " 4280,\n",
       " 4320,\n",
       " 4394,\n",
       " 4460,\n",
       " 4508,\n",
       " 4656,\n",
       " 4675,\n",
       " 4713,\n",
       " 4856,\n",
       " 5017,\n",
       " 5262,\n",
       " 5355,\n",
       " 5356,\n",
       " 5453,\n",
       " 5465,\n",
       " 5467,\n",
       " 5475,\n",
       " 5680,\n",
       " 5852,\n",
       " 5897,\n",
       " 5992,\n",
       " 6001,\n",
       " 6019,\n",
       " 6059,\n",
       " 6086,\n",
       " 6184,\n",
       " 6214,\n",
       " 6364,\n",
       " 6421,\n",
       " 6428,\n",
       " 6739,\n",
       " 6822,\n",
       " 6912,\n",
       " 7170,\n",
       " 7225,\n",
       " 7228,\n",
       " 7252,\n",
       " 7332,\n",
       " 7344,\n",
       " 7352,\n",
       " 7461,\n",
       " 7712,\n",
       " 8284,\n",
       " 8315,\n",
       " 8434,\n",
       " 8623,\n",
       " 8731,\n",
       " 8838,\n",
       " 8845,\n",
       " 8860,\n",
       " 8940,\n",
       " 8942,\n",
       " 8959,\n",
       " 8990,\n",
       " 9104,\n",
       " 9147,\n",
       " 9174,\n",
       " 9255,\n",
       " 9337,\n",
       " 9344,\n",
       " 9344,\n",
       " 9541,\n",
       " 9612,\n",
       " 9623,\n",
       " 9883,\n",
       " 9947,\n",
       " 9948,\n",
       " 10029,\n",
       " 10052,\n",
       " 10079,\n",
       " 10093,\n",
       " 10142,\n",
       " 10151,\n",
       " 10153,\n",
       " 10159,\n",
       " 10236,\n",
       " 10390,\n",
       " 10406,\n",
       " 10481,\n",
       " 10673,\n",
       " 10709,\n",
       " 10710,\n",
       " 11092,\n",
       " 11219,\n",
       " 11276,\n",
       " 11422,\n",
       " 11474,\n",
       " 11515,\n",
       " 11576,\n",
       " 11721,\n",
       " 11839,\n",
       " 11855,\n",
       " 12052,\n",
       " 12200,\n",
       " 12241,\n",
       " 12326,\n",
       " 12333,\n",
       " 12582,\n",
       " 12642,\n",
       " 12687,\n",
       " 12718,\n",
       " 12741,\n",
       " 12783,\n",
       " 12808,\n",
       " 12820,\n",
       " 12900,\n",
       " 13033,\n",
       " 13041,\n",
       " 13042,\n",
       " 13048,\n",
       " 13091,\n",
       " 13141,\n",
       " 13170,\n",
       " 13200,\n",
       " 13211,\n",
       " 13213,\n",
       " 13343,\n",
       " 13483,\n",
       " 13501,\n",
       " 13606,\n",
       " 13615,\n",
       " 13616,\n",
       " 13621,\n",
       " 13686,\n",
       " 13700,\n",
       " 13777,\n",
       " 13805,\n",
       " 13848,\n",
       " 13859,\n",
       " 13947,\n",
       " 13974,\n",
       " 14014,\n",
       " 14079,\n",
       " 14162,\n",
       " 14184,\n",
       " 14198,\n",
       " 14333,\n",
       " 14349,\n",
       " 14367,\n",
       " 14525,\n",
       " 14627,\n",
       " 14740,\n",
       " 14848,\n",
       " 14861,\n",
       " 14906,\n",
       " 14961,\n",
       " 14996,\n",
       " 15016,\n",
       " 15086,\n",
       " 15129,\n",
       " 15396,\n",
       " 15473,\n",
       " 15498,\n",
       " 15686,\n",
       " 15690,\n",
       " 15750,\n",
       " 15779,\n",
       " 15922,\n",
       " 15929,\n",
       " 15966,\n",
       " 15970,\n",
       " 16059,\n",
       " 16244,\n",
       " 16296,\n",
       " 16538,\n",
       " 16590,\n",
       " 16634,\n",
       " 16659,\n",
       " 16814,\n",
       " 16822,\n",
       " 17016,\n",
       " 17056,\n",
       " 17165,\n",
       " 17307,\n",
       " 17318,\n",
       " 17452,\n",
       " 17642,\n",
       " 17708,\n",
       " 17718,\n",
       " 17764,\n",
       " 17910,\n",
       " 17921,\n",
       " 17984,\n",
       " 17994,\n",
       " 18008,\n",
       " 18041,\n",
       " 18096,\n",
       " 18123,\n",
       " 18179,\n",
       " 18242,\n",
       " 18403,\n",
       " 18470,\n",
       " 18516,\n",
       " 18593,\n",
       " 18669,\n",
       " 18683,\n",
       " 18749,\n",
       " 18766,\n",
       " 18913,\n",
       " 19121,\n",
       " 19343,\n",
       " 19355,\n",
       " 19355,\n",
       " 19369,\n",
       " 19450,\n",
       " 19486,\n",
       " 19491,\n",
       " 19546,\n",
       " 19553,\n",
       " 19664,\n",
       " 19672,\n",
       " 19700,\n",
       " 19746,\n",
       " 19758,\n",
       " 19878,\n",
       " 19927,\n",
       " 19956,\n",
       " 19997,\n",
       " 20017,\n",
       " 20057,\n",
       " 20123,\n",
       " 20310,\n",
       " 20356,\n",
       " 20472,\n",
       " 20496,\n",
       " 20497,\n",
       " 20515,\n",
       " 20577,\n",
       " 20646,\n",
       " 20757,\n",
       " 20801,\n",
       " 20811,\n",
       " 20868,\n",
       " 20937,\n",
       " 20997,\n",
       " 21034,\n",
       " 21111,\n",
       " 21122,\n",
       " 21274,\n",
       " 21276,\n",
       " 21291,\n",
       " 21361,\n",
       " 21395,\n",
       " 21412,\n",
       " 21426,\n",
       " 21515,\n",
       " 21566,\n",
       " 21638,\n",
       " 21652,\n",
       " 21692,\n",
       " 21710,\n",
       " 21710,\n",
       " 21779,\n",
       " 21781,\n",
       " 21874,\n",
       " 21960,\n",
       " 21971,\n",
       " 22001,\n",
       " 22002,\n",
       " 22062,\n",
       " 22319,\n",
       " 22422,\n",
       " 22451,\n",
       " 22452,\n",
       " 22481,\n",
       " 22755,\n",
       " 22843,\n",
       " 22903,\n",
       " 23108,\n",
       " 23271,\n",
       " 23319,\n",
       " 23330,\n",
       " 23414,\n",
       " 23470,\n",
       " 23552,\n",
       " 23553,\n",
       " 23618,\n",
       " 23670,\n",
       " 23772,\n",
       " 23913,\n",
       " 24007,\n",
       " 24079,\n",
       " 24100,\n",
       " 24112,\n",
       " 24179,\n",
       " 24197,\n",
       " 24208,\n",
       " 24296,\n",
       " 24406,\n",
       " 24422,\n",
       " 24514,\n",
       " 24520,\n",
       " 24574,\n",
       " 24597,\n",
       " 24626,\n",
       " 24666,\n",
       " 24669,\n",
       " 24717,\n",
       " 24873,\n",
       " 24979,\n",
       " 24986,\n",
       " 25009,\n",
       " 25014,\n",
       " 25043,\n",
       " 25057,\n",
       " 25131,\n",
       " 25160,\n",
       " 25209,\n",
       " 25227,\n",
       " 25309,\n",
       " 25327,\n",
       " 25610,\n",
       " 25689,\n",
       " 25724,\n",
       " 25731,\n",
       " 25742,\n",
       " 25797,\n",
       " 25874,\n",
       " 26003,\n",
       " 26276,\n",
       " 26281,\n",
       " 26339,\n",
       " 26377,\n",
       " 26397,\n",
       " 26451,\n",
       " 26585,\n",
       " 26714,\n",
       " 26825,\n",
       " 26862,\n",
       " 26898,\n",
       " 26905,\n",
       " 26942,\n",
       " 26956,\n",
       " 27036,\n",
       " 27054,\n",
       " 27100,\n",
       " 27134,\n",
       " 27203,\n",
       " 27308,\n",
       " 27518,\n",
       " 27632,\n",
       " 27697,\n",
       " 27749,\n",
       " 27778,\n",
       " 27821,\n",
       " 27979,\n",
       " 28001,\n",
       " 28007,\n",
       " 28009,\n",
       " 28095,\n",
       " 28134,\n",
       " 28149,\n",
       " 28152,\n",
       " 28160,\n",
       " 28301,\n",
       " 28483,\n",
       " 28501,\n",
       " 28613,\n",
       " 28632,\n",
       " 28696,\n",
       " 28699,\n",
       " 28794,\n",
       " 28846,\n",
       " 28924,\n",
       " 28971,\n",
       " 29035,\n",
       " 29119,\n",
       " 29174,\n",
       " 29239,\n",
       " 29265,\n",
       " 29467,\n",
       " 29471,\n",
       " 29546,\n",
       " 29608,\n",
       " 29754,\n",
       " 29755,\n",
       " 29853,\n",
       " 29914,\n",
       " 29948,\n",
       " 30008,\n",
       " 30113,\n",
       " 30159,\n",
       " 30165,\n",
       " 30218,\n",
       " 30254,\n",
       " 30414,\n",
       " 30513,\n",
       " 30528,\n",
       " 30925,\n",
       " 30952,\n",
       " 30997,\n",
       " 31015,\n",
       " 31087,\n",
       " 31295,\n",
       " 31349,\n",
       " 31495,\n",
       " 31539,\n",
       " 31570,\n",
       " 31612,\n",
       " 31621,\n",
       " 31640,\n",
       " 31658,\n",
       " 31679,\n",
       " 31796,\n",
       " 31953,\n",
       " 31964,\n",
       " 32026,\n",
       " 32075,\n",
       " 32083,\n",
       " 32138,\n",
       " 32165,\n",
       " 32289,\n",
       " 32329,\n",
       " 32429,\n",
       " 32506,\n",
       " 32564,\n",
       " 32565,\n",
       " 32622,\n",
       " 32687,\n",
       " 32742,\n",
       " 32756,\n",
       " 32867,\n",
       " 32900,\n",
       " 32976,\n",
       " 33005,\n",
       " 33033,\n",
       " 33127,\n",
       " 33132,\n",
       " 33298,\n",
       " 33316,\n",
       " 33407,\n",
       " 33447,\n",
       " 33483,\n",
       " 33489,\n",
       " 33496,\n",
       " 33537,\n",
       " 33590,\n",
       " 33636,\n",
       " 33665,\n",
       " 33684,\n",
       " 33686,\n",
       " 33701,\n",
       " 33882,\n",
       " 33928,\n",
       " 33950,\n",
       " 33994,\n",
       " 34037,\n",
       " 34049,\n",
       " 34098,\n",
       " 34102,\n",
       " 34153,\n",
       " 34251,\n",
       " 34311,\n",
       " 34312,\n",
       " 34323,\n",
       " 34328,\n",
       " 34331,\n",
       " 34348,\n",
       " 34381,\n",
       " 34386,\n",
       " 34473,\n",
       " 34557,\n",
       " 34660,\n",
       " 34878,\n",
       " 34911,\n",
       " 34932,\n",
       " 35008,\n",
       " 35009,\n",
       " 35041,\n",
       " 35111,\n",
       " 35225,\n",
       " 35241,\n",
       " 35243,\n",
       " 35347,\n",
       " 35368,\n",
       " 35369,\n",
       " 35440,\n",
       " 35502,\n",
       " 35528,\n",
       " 35659,\n",
       " 35673,\n",
       " 35805,\n",
       " 35845,\n",
       " 35857,\n",
       " 35939,\n",
       " 36028,\n",
       " 36095,\n",
       " 36152,\n",
       " 36154,\n",
       " 36172,\n",
       " 36255,\n",
       " 36398,\n",
       " 36863,\n",
       " 36946,\n",
       " 36952,\n",
       " 36985,\n",
       " 37145,\n",
       " 37154,\n",
       " 37179,\n",
       " 37202,\n",
       " 37208,\n",
       " 37236,\n",
       " 37280,\n",
       " 37296,\n",
       " 37446,\n",
       " 37578,\n",
       " 37633,\n",
       " 37657,\n",
       " 37675,\n",
       " 37766,\n",
       " 37814,\n",
       " 37848,\n",
       " 37906,\n",
       " 37940,\n",
       " 38004,\n",
       " 38160,\n",
       " 38269,\n",
       " 38342,\n",
       " 38375,\n",
       " 38528,\n",
       " 38615,\n",
       " 38736,\n",
       " 38739,\n",
       " 38773,\n",
       " 38917,\n",
       " 39086,\n",
       " 39105,\n",
       " 39147,\n",
       " 39189,\n",
       " 39209,\n",
       " 39217,\n",
       " 39225,\n",
       " 39235,\n",
       " 39284,\n",
       " 39328,\n",
       " 39331,\n",
       " 39345,\n",
       " 39356,\n",
       " 39417,\n",
       " 39522,\n",
       " 39560,\n",
       " 39576,\n",
       " 39701,\n",
       " 39726,\n",
       " 39730,\n",
       " 39745,\n",
       " 39777,\n",
       " 39795,\n",
       " 39948,\n",
       " 40223,\n",
       " 40311,\n",
       " 40327,\n",
       " 40398,\n",
       " 40517,\n",
       " 40551,\n",
       " 40582,\n",
       " 40594,\n",
       " 40623,\n",
       " 40680,\n",
       " 40699,\n",
       " 40830,\n",
       " 40887,\n",
       " 40887,\n",
       " 40969,\n",
       " 40989,\n",
       " 41014,\n",
       " 41206,\n",
       " 41206,\n",
       " 41326,\n",
       " 41412,\n",
       " 41488,\n",
       " 41488,\n",
       " 41515,\n",
       " 41702,\n",
       " 41704,\n",
       " 41728,\n",
       " 41728,\n",
       " 41779,\n",
       " 41791,\n",
       " 41837,\n",
       " 42020,\n",
       " 42058,\n",
       " 42117,\n",
       " 42151,\n",
       " 42184,\n",
       " 42264,\n",
       " 42275,\n",
       " 42328,\n",
       " 42342,\n",
       " 42518,\n",
       " 42668,\n",
       " 42702,\n",
       " 42833,\n",
       " 43003,\n",
       " 43136,\n",
       " 43242,\n",
       " 43338,\n",
       " 43450,\n",
       " 43457,\n",
       " 43564,\n",
       " 43645,\n",
       " 43767,\n",
       " 43770,\n",
       " 43994,\n",
       " 44047,\n",
       " 44119,\n",
       " 44240,\n",
       " 44249,\n",
       " 44254,\n",
       " 44310,\n",
       " 44466,\n",
       " 44484,\n",
       " 44611,\n",
       " 44660,\n",
       " 44727,\n",
       " 44749,\n",
       " 44774,\n",
       " 44808,\n",
       " 44850,\n",
       " 45192,\n",
       " 45238,\n",
       " 45328,\n",
       " 45413,\n",
       " 45595,\n",
       " 45735,\n",
       " 45869,\n",
       " 45871,\n",
       " 45941,\n",
       " 46048,\n",
       " 46085,\n",
       " 46091,\n",
       " 46119,\n",
       " 46141,\n",
       " 46192,\n",
       " 46221,\n",
       " 46269,\n",
       " 46344,\n",
       " 46421,\n",
       " 46457,\n",
       " 46475,\n",
       " 46482,\n",
       " 46491,\n",
       " 46545,\n",
       " 46603,\n",
       " 46691,\n",
       " 46773,\n",
       " 46811,\n",
       " 46887,\n",
       " 46918,\n",
       " 46991,\n",
       " 46999,\n",
       " 47124,\n",
       " 47227,\n",
       " 47271,\n",
       " 47342,\n",
       " 47389,\n",
       " 47456,\n",
       " 47678,\n",
       " 47703,\n",
       " 47778,\n",
       " 47999,\n",
       " 48010,\n",
       " 48049,\n",
       " 48050,\n",
       " 48137,\n",
       " 48189,\n",
       " 48302,\n",
       " 48355,\n",
       " 48452,\n",
       " 48471,\n",
       " 48569,\n",
       " 48599,\n",
       " 48720,\n",
       " 48773,\n",
       " 48779,\n",
       " 48990,\n",
       " 49088,\n",
       " 49097,\n",
       " 49098,\n",
       " 49235,\n",
       " 49277,\n",
       " 49542,\n",
       " 49707,\n",
       " 49900,\n",
       " 50061,\n",
       " 50098,\n",
       " 50360,\n",
       " 50412,\n",
       " 50495,\n",
       " 50497,\n",
       " 50523,\n",
       " 50544,\n",
       " 50584,\n",
       " 50661,\n",
       " 50668,\n",
       " 50742,\n",
       " 50777,\n",
       " 50845,\n",
       " 50846,\n",
       " 50876,\n",
       " 50911,\n",
       " 50932,\n",
       " 50964,\n",
       " 51023,\n",
       " 51055,\n",
       " 51443,\n",
       " 51476,\n",
       " 51611,\n",
       " 51644,\n",
       " 51717,\n",
       " 51801,\n",
       " 51875,\n",
       " 51908,\n",
       " 52061,\n",
       " 52088,\n",
       " 52109,\n",
       " 52161,\n",
       " 52162,\n",
       " 52174,\n",
       " 52208,\n",
       " 52236,\n",
       " 52307,\n",
       " 52461,\n",
       " 52493,\n",
       " 52533,\n",
       " 52820,\n",
       " 53049,\n",
       " 53073,\n",
       " 53199,\n",
       " 53262,\n",
       " 53313,\n",
       " 53417,\n",
       " 53554,\n",
       " 53611,\n",
       " 53623,\n",
       " 53700,\n",
       " 53741,\n",
       " 53852,\n",
       " 53901,\n",
       " 54011,\n",
       " 54029,\n",
       " 54122,\n",
       " 54289,\n",
       " 54369,\n",
       " 54603,\n",
       " 54739,\n",
       " 54856,\n",
       " 54882,\n",
       " 54953,\n",
       " 54955,\n",
       " 54958,\n",
       " 55146,\n",
       " 55172,\n",
       " 55261,\n",
       " 55276,\n",
       " 55328,\n",
       " 55333,\n",
       " 55349,\n",
       " 55379,\n",
       " 55412,\n",
       " 55439,\n",
       " 55669,\n",
       " 55752,\n",
       " 55754,\n",
       " 55817,\n",
       " 55836,\n",
       " 55858,\n",
       " 55946,\n",
       " 55952,\n",
       " 55956,\n",
       " 56024,\n",
       " 56067,\n",
       " 56308,\n",
       " 56512,\n",
       " 56530,\n",
       " 56541,\n",
       " 56558,\n",
       " 56740,\n",
       " 56795,\n",
       " 56806,\n",
       " 56965,\n",
       " 57045,\n",
       " 57333,\n",
       " 57377,\n",
       " 57711,\n",
       " 57729,\n",
       " 57745,\n",
       " 57745,\n",
       " 57879,\n",
       " 57896,\n",
       " 57915,\n",
       " 57991,\n",
       " 58181,\n",
       " 58272,\n",
       " 58280,\n",
       " 58296,\n",
       " 58449,\n",
       " 58486,\n",
       " 58678,\n",
       " 58687,\n",
       " 58733,\n",
       " 58781,\n",
       " 58850,\n",
       " 58860,\n",
       " 58884,\n",
       " 58964,\n",
       " 59004,\n",
       " 59052,\n",
       " 59178,\n",
       " 59247,\n",
       " 59405,\n",
       " 59527,\n",
       " 59557,\n",
       " 59560,\n",
       " 59669,\n",
       " 59697,\n",
       " 59818,\n",
       " 59883,\n",
       " 59902,\n",
       " 59977,\n",
       " 60115,\n",
       " 60125,\n",
       " 60369,\n",
       " 60370,\n",
       " 60406,\n",
       " 60429,\n",
       " 60450,\n",
       " 60489,\n",
       " 60546,\n",
       " 60560,\n",
       " 60630,\n",
       " 60842,\n",
       " 60969,\n",
       " 61047,\n",
       " 61557,\n",
       " 61708,\n",
       " 61731,\n",
       " 61805,\n",
       " 61808,\n",
       " 61905,\n",
       " 61910,\n",
       " 61932,\n",
       " 62042,\n",
       " 62069,\n",
       " 62082,\n",
       " 62082,\n",
       " 62101,\n",
       " 62254,\n",
       " 62283,\n",
       " 62339,\n",
       " 62346,\n",
       " 62390,\n",
       " 62404,\n",
       " 62417,\n",
       " 62458,\n",
       " 62481,\n",
       " 62535,\n",
       " 62593,\n",
       " 62628,\n",
       " 62667,\n",
       " 62937,\n",
       " 63075,\n",
       " 63085,\n",
       " 63093,\n",
       " 63112,\n",
       " 63115,\n",
       " 63199,\n",
       " 63260,\n",
       " 63289,\n",
       " 63318,\n",
       " 63340,\n",
       " 63342,\n",
       " 63680,\n",
       " 63692,\n",
       " 63721,\n",
       " 63727,\n",
       " 63836,\n",
       " 63850,\n",
       " 63915,\n",
       " 63918,\n",
       " 63969,\n",
       " 64017,\n",
       " 64255,\n",
       " 64255,\n",
       " 64257,\n",
       " 64387,\n",
       " 64460,\n",
       " 64481,\n",
       " 64483,\n",
       " 64484,\n",
       " 64502,\n",
       " 64607,\n",
       " 64610,\n",
       " 64655,\n",
       " 64681,\n",
       " 64713,\n",
       " 64714,\n",
       " 64803,\n",
       " 64824,\n",
       " 64893,\n",
       " 64925,\n",
       " 64935,\n",
       " 65167,\n",
       " 65213,\n",
       " 65216,\n",
       " 65258,\n",
       " 65260,\n",
       " 65265,\n",
       " 65308,\n",
       " 65414,\n",
       " 65453,\n",
       " 65474,\n",
       " 65698,\n",
       " 65799,\n",
       " 65838,\n",
       " 65862,\n",
       " 65886,\n",
       " 65892,\n",
       " 65903,\n",
       " 66073,\n",
       " 66103,\n",
       " 66158,\n",
       " 66165,\n",
       " 66287,\n",
       " 66451,\n",
       " 66452,\n",
       " 66480,\n",
       " 66543,\n",
       " 66564,\n",
       " 66691,\n",
       " 66834,\n",
       " 66941,\n",
       " 67013,\n",
       " 67030,\n",
       " 67039,\n",
       " 67131,\n",
       " 67131,\n",
       " 67280,\n",
       " 67294,\n",
       " 67323,\n",
       " 67357,\n",
       " 67384,\n",
       " 67417,\n",
       " ...]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in a if col > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 23\n",
      "1 45\n",
      "2 0\n"
     ]
    }
   ],
   "source": [
    "class Solution:\n",
    "    def threeSum(self, nums: List[int]) -> List[List[int]]:\n",
    "        i = 0\n",
    "        j = 1\n",
    "        res_val = []\n",
    "        if len(nums) < 3:\n",
    "            return []\n",
    "        x_y_tracker = []\n",
    "        #nums = list(set(nums))\n",
    "        nums.sort()\n",
    "        search_index = {z:z1 for z,z1 in zip(nums,range(0,len(nums)))}\n",
    "        x_tracker = []\n",
    "        for i,x in enumerate(nums,0):\n",
    "            if x > 0:\n",
    "                print(x)\n",
    "                break\n",
    "            if x not in x_tracker:\n",
    "                x_tracker.append(x)\n",
    "                #while j <= len(nums):\n",
    "                for y in nums[j:]:\n",
    "                #for j,y in enumerate(nums[j:],1):\n",
    "                    tmpval = -(x+y)\n",
    "                    if tmpval in search_index and search_index[tmpval] not in [i,j]:\n",
    "                        res_val.append(sorted([x,y,tmpval]))\n",
    "                    j = j+1\n",
    "            #i = i+1\n",
    "            j = i+2\n",
    "        res_val1 = []\n",
    "        [res_val1.append(val) for val in res_val if val not in res_val1]\n",
    "        #print(ret_val)\n",
    "        return res_val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def threeSum(self, nums: List[int]) -> List[List[int]]:\n",
    "        i = 0\n",
    "        j = 1\n",
    "        res_val = []\n",
    "        if len(nums) < 3:\n",
    "            return []\n",
    "        x_y_tracker = []\n",
    "        #nums = list(set(nums))\n",
    "        nums.sort()\n",
    "        list_length = len(nums)\n",
    "        search_index = {z:z1 for z,z1 in zip(nums,range(0,len(nums)))}\n",
    "        x_tracker = []\n",
    "        for i in range(0,list_length):\n",
    "        #for i,x in enumerate(nums,0):\n",
    "            if nums[i] > 0:\n",
    "                print(x)\n",
    "                break\n",
    "            if x not in x_tracker:\n",
    "                x_tracker.append(nums[i])\n",
    "                #while j <= len(nums):\n",
    "                for y in nums[j:]:\n",
    "                #for j,y in enumerate(nums[j:],1):\n",
    "                    tmpval = -(nums[i]+y)\n",
    "                    if tmpval in search_index and search_index[tmpval] not in [i,j]:\n",
    "                        res_val.append(sorted([nums[i],y,tmpval]))\n",
    "                    j = j+1\n",
    "            #i = i+1\n",
    "            j = i+2\n",
    "        res_val1 = []\n",
    "        [res_val1.append(val) for val in res_val if val not in res_val1]\n",
    "        #print(ret_val)\n",
    "        return res_val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class Solution:\n",
    "    def lengthOfLongestSubstring(self, s: str) -> int:\n",
    "        s = s.replace(\" \",'~')\n",
    "        long_char = []\n",
    "        actual_val = 0\n",
    "        i =1\n",
    "        j = 0\n",
    "        if len(s) > 1:\n",
    "            for j in range(0,len(s)):\n",
    "                dict_val = {}\n",
    "                for i in range(1,len(s)+1):                    \n",
    "                    if len(list(OrderedDict.fromkeys(s[j:i]))) == len(s[j:i]):\n",
    "                        if len(s[j:i]) > actual_val:\n",
    "                               actual_val = len(s[j:i])\n",
    "                    else:\n",
    "                        break\n",
    "                    i = i+1\n",
    "                j = j+1\n",
    "        elif len(s) == 0:\n",
    "            actual_val = 0\n",
    "        else:\n",
    "            actual_val = 1\n",
    "        return actual_val\n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '''abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "'''\n",
    "b= '''abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}\n",
    "'''\n",
    "a = a.replace(\"\\n\", \"~\")\n",
    "print(a)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict,Counter\n",
    "import numpy as np\n",
    "class Solution:\n",
    "    def lengthOfLongestSubstring(self, s: str) -> int:\n",
    "        max_len = 0\n",
    "        full_len = len(s)\n",
    "        tracker = {}\n",
    "        if full_len == 1:\n",
    "            max_len = 1\n",
    "        elif full_len == 2:\n",
    "            if s[0] == s[1]:\n",
    "                max_len = 1\n",
    "            else:\n",
    "                max_len = 2\n",
    "        elif full_len > 1:\n",
    "            for i in range(len(s)):                \n",
    "                for j in range(i+1,len(s)+1):\n",
    "                    tmpval = s[i:j]\n",
    "                    if tmpval not in tracker:\n",
    "                        tracker.update({tmpval:''})\n",
    "                        len_val = len({a:'' for a in tmpval})\n",
    "                        #len_val = len(list(set(s[i:j])))\n",
    "                        len_val1 = len(tmpval)\n",
    "                        if len_val != len_val1:\n",
    "                            break\n",
    "                        if len_val > max_len and len_val == len_val1:\n",
    "                            \n",
    "                            max_len = len_val1\n",
    "        return max_len\n",
    "\n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len({1: 'p', 2: 'w'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Solution:\n",
    "    def split(self,s1,strlen):\n",
    "        self.ind = self.ind+1\n",
    "        r = random.randint(1, strlen)\n",
    "        x = s1[:r]\n",
    "        y = s1[r:]\n",
    "        \n",
    "        return x,y\n",
    "    \n",
    "    def split_value(self,s1):\n",
    "        len_s1 = len(s1)\n",
    "        #a,b = self.split(s1,len_s1) \n",
    "        if random.randint(1, 2) == 1:\n",
    "            a,b = self.split(s1,len_s1)\n",
    "        else:\n",
    "            b,a = self.split(s1,len_s1)\n",
    "        print(self.ind ,21,self.text,1,a,1,b)\n",
    "        if len(a) > 1: \n",
    "            self.split_value(a)\n",
    "        else:\n",
    "            #self.text_list.append(a)\n",
    "            #self.text = self.text  + x if random.randint(1, 2) == 1 else x + self.text \n",
    "            self.text = self.text + a\n",
    "        if len(b) > 1:\n",
    "            self.split_value(b)\n",
    "        else :\n",
    "            #self.text_list.append(b)\n",
    "            #self.text = self.text  + y if random.randint(1, 2) == 1 else y + self.text \n",
    "            self.text = self.text + b\n",
    "        #print(2,a,b,self.text)\n",
    "        #if len(a)==1 and len(b)==1  :\n",
    "        #    self.text = self.text  + a + b if random.randint(1, 2) == 1 else self.text + b + a\n",
    "        #if len(b)==1:\n",
    "         #   self.text =  self.text + b\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.text = ''\n",
    "        \n",
    "    def isScramble(self, s1: str, s2: str) -> bool:\n",
    "        \n",
    "        self.ind = 0\n",
    "        self.text_list = []\n",
    "        #if self.text != s2:     \n",
    "            #self.text = ''\n",
    "            #self.isScramble(s1,s2)\n",
    "            #print(self.text,s2,s1)\n",
    "            #return False\n",
    "        #else:\n",
    "            #return True\n",
    "        \n",
    "        self.split_value(s1)\n",
    "        print(self.text,s2,s1)\n",
    "        if self.text == s2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'catsanddog'\n",
    "wordDict = [\"cat\",\"cats\",\"and\",\"sand\",\"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'dog']"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'catsanddog'.split('cat')\n",
    "'sanddog'.split('and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-523-3f97bedb2582>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "['s','a','n','d','y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(c)):\n",
    "    if i > len(c) - len(d):\n",
    "        break\n",
    "    #print(np.array(c[i:len(d)+i]))\n",
    "    #print(np.array(d))\n",
    "    t = np.array(c[i:len(d)+i]) == np.array(d)\n",
    "    print(sum(t))\n",
    "    #print(np.array(c[i:len(d)+i]))\n",
    "    #print(t,len(d)+i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [[0,0,0],[1,1,0],[1,1,0]]\n",
    "\n",
    "x = [{i:j} for sublist in grid for i,j in zip(sublist,range(len(sublist))) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0}, {0: 2}, {0: 2}]"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{0: 0}, {0: 2}], [{0: 2}, {0: 2}], [{0: 2}, {0: 2}]]"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_index = []\n",
    "for val in grid:\n",
    "    subgrid = {}\n",
    "    for v1,i in zip(reversed(val),reversed(range(len(val)))):\n",
    "        #print(i)\n",
    "        if v1 == 0:\n",
    "            subgrid.update({v1:i})\n",
    "    least_index.append(subgrid)\n",
    "greatest_index = []\n",
    "for val in grid:\n",
    "    subgrid = {}\n",
    "    for v1,i in zip(val,range(len(val))):\n",
    "        if v1 == 0:\n",
    "            subgrid.update({v1:i})\n",
    "    greatest_index.append(subgrid)\n",
    "index_list = [[x,y] for x,y in zip(least_index,greatest_index)]\n",
    "if len(grid) == len(greatest_index):\n",
    "    pnt = index_list[0][0][0]\n",
    "    for i,j in zip(range(0,len(grid)+1),range(1,len(grid))):\n",
    "        print(index_list[i][0][0],index_list[i][1][0])\n",
    "        while pnt < index_list[i][1][0] and pnt <= index_list[j][0][0]:\n",
    "            pnt = pnt + 1\n",
    "        if index_list[i][0][0] <= index_list[j][0][0]:\n",
    "            pnt = pnt + 1\n",
    "else:\n",
    "    pnt = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_palindrome(strval):\n",
    "    lenval = len(strval)\n",
    "    i =lenval//2\n",
    "    i_h = lenval%2\n",
    "    retval = [-1,-1]\n",
    "    if i_h > 0:\n",
    "        left = i-1\n",
    "        right=i+1\n",
    "    else:\n",
    "        left = i-1\n",
    "        right = i\n",
    "    while right <= lenval-1:\n",
    "        if strval[left] == strval[right]:\n",
    "            retval = [left,right]\n",
    "            left = left-1\n",
    "            right = right+1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if retval[0]!=-1:\n",
    "        return strval[retval[0]:retval[1]+1]\n",
    "    else:\n",
    "        return '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_string(s):\n",
    "    palindrome =[]\n",
    "    for i in range(0,len(s)):\n",
    "        for j in range(0,len(s)):\n",
    "            l = get_palindrome(s[i:j+1])\n",
    "            if l!='':\n",
    "                palindrome.append(l)\n",
    "    return max(palindrome, key = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aba'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = trace_string(s='aaba')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_palindrome('aaba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('exex')//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[1, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_palindrome('babad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0} {0: 2}\n",
      "{0: 2} {0: 2}\n",
      "{0: 2} {0: 2}\n"
     ]
    }
   ],
   "source": [
    "curr_zero_max_index = -1\n",
    "curr_zero_min_index = -1\n",
    "route =[]\n",
    "for l,g in zip(least_index,greatest_index):\n",
    "    if 0 in g:\n",
    "        curr_zero_max_index = g[0]\n",
    "    if 0 in l:\n",
    "        curr_zero_min_index = l[0]\n",
    "        if l[0] > curr_zero_min_index:\n",
    "            route.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0}\n",
      "{0: 2}\n",
      "{0: 2}\n"
     ]
    }
   ],
   "source": [
    "route = []\n",
    "curr_zero_min_index = -1\n",
    "greatest_index =-1\n",
    "for x in range(0,grid):\n",
    "    if 0 in least_index[i]:\n",
    "        curr_zero_min_index = least_index[i][0]\n",
    "        curr_zero_max_index = greatest_index[i][0]\n",
    "        if least_index[i]\n",
    "        route.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'int' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-594-934ae49fca79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mzero_greatest_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
     ]
    }
   ],
   "source": [
    "final_all = []\n",
    "cnt = 0\n",
    "index = len(final)\n",
    "for x,i in zip(grid,range(len(grid))):\n",
    "    for y,j in zip(grid,range(len(x))):\n",
    "        if 0 in x[i]:\n",
    "            zero_greatest_index = x[i][0]\n",
    "        \n",
    "        \n",
    "    if 0 in x and x[0] < index:\n",
    "        cnt = cnt+1\n",
    "        index = x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not reversible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-538-016fec27e147>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m}\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not reversible"
     ]
    }
   ],
   "source": [
    "[{i:j}   for i,j in zip(reversed(val),len(val)) for val in grid ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "class Solution:\n",
    "    def lengthOfLongestSubstring(self, s: str) -> int:\n",
    "        s = s.replace(\" \",'~')\n",
    "        long_char = []\n",
    "        actual_val = 0\n",
    "        i =1\n",
    "        j = 0\n",
    "        \n",
    "        if len(s) > 1:\n",
    "            for i in reversed(range(len(s)-1)):\n",
    "                duplicate_ind= 0\n",
    "                d= s[:i+1]\n",
    "                print('INPU',d)\n",
    "                for i in range(0,len(s)):\n",
    "                    if i > len(s) - len(d):\n",
    "                        break\n",
    "                    \n",
    "                    #print(s)\n",
    "                    #print(np.array(s[i:len(d)+i]),'----',np.array(d))\n",
    "                    #t = np.array(s[i:len(d)+i]) == np.array(d)\n",
    "                    print(s[i:len(d)+i])\n",
    "                    t = s[i:len(d)+i] == d\n",
    "                    #print(t)\n",
    "                    if t is True:\n",
    "                        duplicate_ind = duplicate_ind + 1\n",
    "                    if duplicate_ind> 1:\n",
    "                        return len(list(set(d)))\n",
    "        return duplicate_ind\n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(d+[0]*(len(c)-len(d)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 4, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(c) - np.array(d+[0]*(len(c)-len(d)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, -2, -1, 1, 2]\n",
      "VALS 3 0\n",
      "{-2: 2, -1: 3, 1: 4, 2: 5}\n",
      "VALS 3 -2\n",
      "{0: 1, -1: 3, 1: 4, 2: 5}\n",
      "VALS 3 -1\n",
      "{0: 1, -2: 2, 1: 4, 2: 5}\n",
      "VALS 3 1\n",
      "{0: 1, -2: 2, -1: 3, 2: 5}\n",
      "VALS 3 2\n",
      "{0: 1, -2: 2, -1: 3, 1: 4}\n"
     ]
    }
   ],
   "source": [
    "nums = [3,0,-2,-1,1,2]\n",
    "i = 0\n",
    "j = 1\n",
    "print(nums)\n",
    "ret_val = []\n",
    "res_val = []\n",
    "for x in nums:\n",
    "    search_index = {z:z1 for z,z1 in zip(nums,range(0,len(nums)))}\n",
    "    for y in nums[j:]:\n",
    "        tmpval = x+y\n",
    "        search_index_tmp = {t:t1 for t,t1 in search_index.items() if t1 not in [i,j]}\n",
    "        print('VALS',x,y)\n",
    "        print(search_index_tmp)\n",
    "        if -tmpval in search_index_tmp:\n",
    "            r = sorted([x,y,-tmpval])\n",
    "            if r not in res_val:\n",
    "                res_val.append(r)\n",
    "        j = j+1\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "3 0\n",
      "3 -2\n",
      "3 -1\n",
      "3 1\n",
      "3 2\n",
      "-----\n",
      "0 -2\n",
      "0 -1\n",
      "0 1\n",
      "0 2\n",
      "-----\n",
      "-2 -1\n",
      "-2 1\n",
      "-2 2\n",
      "-----\n",
      "-1 1\n",
      "-1 2\n",
      "-----\n",
      "1 2\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "nums = [3,0,-2,-1,1,2]\n",
    "i = 0\n",
    "j = 1\n",
    "for x in nums:\n",
    "    print('-----')\n",
    "    t = nums[j:]\n",
    "    for y in t:\n",
    "        print(x,y)\n",
    "        j = j+1\n",
    "    \n",
    "    i = i+1\n",
    "    j = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD 1 1 4 3\n",
      "ZZZZ 8115\n",
      "ZZZZ 7992\n",
      "ZZZZ 7869\n",
      "ZZZZ 7746\n",
      "ZZZZ 7623\n",
      "ZZZZ 7500\n",
      "ZZZZ 7377\n",
      "ZZZZ 7254\n",
      "ZZZZ 7131\n",
      "ZZZZ 7008\n",
      "ZZZZ 6885\n",
      "ZZZZ 6762\n",
      "ZZZZ 6639\n",
      "ZZZZ 6516\n",
      "ZZZZ 6393\n",
      "ZZZZ 6270\n",
      "ZZZZ 6147\n",
      "ZZZZ 6024\n",
      "ZZZZ 5901\n",
      "ZZZZ 5778\n",
      "ZZZZ 5655\n",
      "ZZZZ 5532\n",
      "ZZZZ 5409\n",
      "ZZZZ 5286\n",
      "ZZZZ 5163\n",
      "ZZZZ 5040\n",
      "ZZZZ 4917\n",
      "ZZZZ 4794\n",
      "ZZZZ 4671\n",
      "ZZZZ 4548\n",
      "ZZZZ 4425\n",
      "ZZZZ 4302\n",
      "ZZZZ 4179\n",
      "ZZZZ 4056\n",
      "ZZZZ 3933\n",
      "ZZZZ 3810\n",
      "ZZZZ 3687\n",
      "ZZZZ 3564\n",
      "ZZZZ 3441\n",
      "ZZZZ 3318\n",
      "ZZZZ 3195\n",
      "ZZZZ 3072\n",
      "ZZZZ 2949\n",
      "ZZZZ 2826\n",
      "ZZZZ 2703\n",
      "ZZZZ 2580\n",
      "ZZZZ 2457\n",
      "ZZZZ 2334\n",
      "ZZZZ 2211\n",
      "ZZZZ 2088\n",
      "ZZZZ 1965\n",
      "ZZZZ 1842\n",
      "ZZZZ 1719\n",
      "ZZZZ 1596\n",
      "ZZZZ 1473\n",
      "ZZZZ 1350\n",
      "ZZZZ 1227\n",
      "ZZZZ 1104\n",
      "ZZZZ 981\n",
      "ZZZZ 858\n",
      "ZZZZ 735\n",
      "ZZZZ 612\n",
      "ZZZZ 489\n",
      "ZZZZ 366\n",
      "ZZZZ 243\n",
      "ZZZZ 120\n",
      "ZZZZ -3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66, 120)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1, v2 = divider(v2,dsr)\n",
    "v1,v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 120)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1, v2 = divider(v2,dsr)\n",
    "v1,v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1, v2 = divider(v2,dsr)\n",
    "v1,v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "int(math.log10(multiplier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6150000000000000000000"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123*5*multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(str(div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(12345) - (23*5*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "10 -130 23\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Solution()\n",
    "t.divide(100,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dividend - 400*23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9200"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dividend = 9891\n",
    "divisor = 23\n",
    "\n",
    "23*100*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9720/324 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_val1 = val_reducer(dividend=171,divisor=324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def divide(self, dividend: int, divisor: int) -> int:\n",
    "        val = 0\n",
    "        i = 0\n",
    "        if dividend <0 and divisor < 0:\n",
    "            sign = 'positive'\n",
    "        elif dividend >0 and divisor < 0:\n",
    "            sign = 'negative'\n",
    "        elif dividend <0 and divisor > 0:\n",
    "            sign = 'negative'\n",
    "        else:\n",
    "            sign = 'positive'\n",
    "        dividend = abs(dividend)\n",
    "        divisor = abs(divisor)\n",
    "        #print(dividend,divisor,sign)\n",
    "        while val <= dividend:\n",
    "            val = val + divisor\n",
    "            if val > dividend:\n",
    "                break\n",
    "            i = i+1\n",
    "        \n",
    "        return -i if sign=='negative' else i\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86b787b86fc8737571dbbe615f9354f8802ff0be8a4942a430c73649cedfecfe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('ee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
